<h2 align="center">
    <a href="https://dainam.edu.vn/vi/khoa-cong-nghe-thong-tin">
    ğŸ“  FACULTY OF INFORMATION TECHNOLOGY (DAINAM UNIVERSITY)
    </a>
</h2>
<h2 align="center">
    Há»† THá»NG TÃŒM KIáº¾M HÃŒNH áº¢NH AI
</h2>

<div align="center">
    <p align="center">
        <img src="docs/aiotlab_logo.png" alt="AIoTLab Logo" width="170"/>
        <img src="docs/fitdnu_logo.png" alt="FIT DNU Logo" width="180"/>
        <img src="docs/dnu_logo.png" alt="DaiNam University Logo" width="200"/>
    </p>

[![AIoTLab](https://img.shields.io/badge/AIoTLab-green?style=for-the-badge)](https://www.facebook.com/DNUAIoTLab)
[![Faculty of Information Technology](https://img.shields.io/badge/Faculty%20of%20Information%20Technology-blue?style=for-the-badge)](https://dainam.edu.vn/vi/khoa-cong-nghe-thong-tin)
[![DaiNam University](https://img.shields.io/badge/DaiNam%20University-orange?style=for-the-badge)](https://dainam.edu.vn)

</div>

---

## ğŸ“– 1. Giá»›i thiá»‡u há»‡ thá»‘ng

Há»‡ thá»‘ng **TÃ¬m kiáº¿m HÃ¬nh áº£nh AI** Ä‘Æ°á»£c xÃ¢y dá»±ng dá»±a trÃªn mÃ´ hÃ¬nh **OpenCLIP Ä‘a ngÃ´n ngá»¯ (xlm-roberta-base-ViT-B-32)** nháº±m:

- Há»— trá»£ tÃ¬m kiáº¿m hÃ¬nh áº£nh ngÆ°á»i Viá»‡t Nam thÃ´ng qua mÃ´ táº£ vÄƒn báº£n báº±ng **Tiáº¿ng Viá»‡t** hoáº·c **Tiáº¿ng Anh**.
- Cung cáº¥p giao diá»‡n web hiá»‡n Ä‘áº¡i, thÃ¢n thiá»‡n vá»›i **Gradio**, cho phÃ©p ngÆ°á»i dÃ¹ng nháº­p mÃ´ táº£ vÃ  nháº­n káº¿t quáº£ tá»©c thÃ¬.
- Fine-tune model vá»›i **6686+ captions tiáº¿ng Viá»‡t** Ä‘á»ƒ cáº£i thiá»‡n Ä‘á»™ chÃ­nh xÃ¡c trong ngá»¯ cáº£nh vÄƒn hÃ³a Viá»‡t Nam.

âœ¨ CÃ¡c chá»©c nÄƒng chÃ­nh:

- **Fine-tuning vá»›i Captions**: Huáº¥n luyá»‡n model vá»›i dá»¯ liá»‡u captions tiáº¿ng Viá»‡t Ä‘á»ƒ hiá»ƒu ngá»¯ cáº£nh tá»‘t hÆ¡n.
- **TÃ¬m kiáº¿m thÃ´ng minh**: Nháº­p mÃ´ táº£ chi tiáº¿t (Ã¡o dÃ i, nÃ³n lÃ¡, ngÆ° dÃ¢n, chá»£...) vÃ  nháº­n áº£nh phÃ¹ há»£p nháº¥t.
- **Embedding Cache**: LÆ°u trá»¯ embeddings Ä‘á»ƒ tÄƒng tá»‘c Ä‘á»™ tÃ¬m kiáº¿m.
- **ÄÃ¡nh giÃ¡ Model**: CÃ´ng cá»¥ evaluation vá»›i confusion matrix, accuracy report.

ğŸ¯ Má»¥c tiÃªu há»‡ thá»‘ng:

- **Sá»‘ hÃ³a tra cá»©u hÃ¬nh áº£nh**: Thay tháº¿ tÃ¬m kiáº¿m thá»§ cÃ´ng báº±ng AI thÃ´ng minh.
- **Tá»‘i Æ°u tráº£i nghiá»‡m**: Giao diá»‡n trá»±c quan, káº¿t quáº£ tá»©c thÃ¬ vá»›i Ä‘iá»ƒm similarity.
- **Há»— trá»£ Ä‘a ngÃ´n ngá»¯**: Tiáº¿ng Viá»‡t vÃ  Tiáº¿ng Anh mÃ  khÃ´ng cáº§n dá»‹ch thuáº­t.

## ï¿½ 2. CÃ¡c cÃ´ng nghá»‡ Ä‘Æ°á»£c sá»­ dá»¥ng

- **NgÃ´n ngá»¯:** Python 3.9+
- **MÃ´ hÃ¬nh AI:** OpenCLIP (xlm-roberta-base-ViT-B-32)
- **Framework:** PyTorch, Transformers
- **Giao diá»‡n:** Gradio Web UI
- **PhÃ¢n tÃ­ch:** NumPy, scikit-learn, seaborn, matplotlib
- **Fine-tuning:** Contrastive Learning vá»›i image-caption pairs

<div align="center">

[![Python](https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white)](https://www.python.org/)
[![PyTorch](https://img.shields.io/badge/PyTorch-EE4C2C?style=for-the-badge&logo=pytorch&logoColor=white)](https://pytorch.org/)
[![OpenCLIP](https://img.shields.io/badge/OpenCLIP-412991?style=for-the-badge&logo=openai&logoColor=white)](https://github.com/mlfoundations/open_clip)
[![Gradio](https://img.shields.io/badge/Gradio-FF6F00?style=for-the-badge&logo=gradio&logoColor=white)](https://gradio.app/)
[![NumPy](https://img.shields.io/badge/NumPy-013243?style=for-the-badge&logo=numpy&logoColor=white)](https://numpy.org/)

</div>

## ğŸ“ 3. Cáº¥u trÃºc thÆ° má»¥c

```
ChuyenDoiSo/
â”œâ”€â”€ app.py                          # ğŸŒ á»¨ng dá»¥ng tÃ¬m kiáº¿m Gradio
â”œâ”€â”€ train.py                        # ğŸ”¥ Fine-tuning & tÃ­nh embeddings
â”œâ”€â”€ evaluate_model.py               # ğŸ“Š ÄÃ¡nh giÃ¡ model vá»›i confusion matrix
â”œâ”€â”€ models/
â”‚   â””â”€â”€ clip_model.py               # Wrapper CLIP encode áº£nh & text
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ data_loader.py              # Quáº£n lÃ½ metadata & embedding
â”‚   â”œâ”€â”€ search_engine.py            # Logic tÃ¬m kiáº¿m vÃ  cache
â”‚   â””â”€â”€ visualizer.py               # CÃ´ng cá»¥ trá»±c quan hoÃ¡ káº¿t quáº£
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ captions_draft.json         # 6686 captions tiáº¿ng Viá»‡t cho fine-tuning
â”‚   â””â”€â”€ processed/                  # 3003 áº£nh ngÆ°á»i Viá»‡t Nam
â”œâ”€â”€ trained_models/
â”‚   â”œâ”€â”€ image_embeddings.pkl        # Embedding áº£nh sau training
â”‚   â”œâ”€â”€ fine_tuned_clip_latest.pt   # Model weights sau fine-tuning
â”‚   â”œâ”€â”€ text_embeddings_cache.pkl   # Cache query phá»• biáº¿n
â”‚   â””â”€â”€ training_config.json        # Cáº¥u hÃ¬nh training
â”œâ”€â”€ docs/                           # Logo vÃ  hÃ¬nh áº£nh minh há»a
â”œâ”€â”€ requirements.txt                # Danh sÃ¡ch dependencies
â””â”€â”€ README.md                       # TÃ i liá»‡u dá»± Ã¡n
```

> ğŸ’¡ **LÆ°u Ã½**: File `captions_draft.json` chá»©a 6686 captions tiáº¿ng Viá»‡t Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ fine-tune model, giÃºp cáº£i thiá»‡n Ä‘á»™ chÃ­nh xÃ¡c trong ngá»¯ cáº£nh vÄƒn hÃ³a Viá»‡t Nam.

## âš™ï¸ 4. CÃ¡c bÆ°á»›c cÃ i Ä‘áº·t & sá»­ dá»¥ng

### 1ï¸âƒ£ Chuáº©n bá»‹ mÃ´i trÆ°á»ng

- CÃ i Ä‘áº·t **Python 3.9+** â†’ [Táº£i táº¡i Ä‘Ã¢y](https://www.python.org/downloads/)
- CÃ i Ä‘áº·t **Git** (optional) â†’ [Táº£i táº¡i Ä‘Ã¢y](https://git-scm.com/downloads)
- Há»‡ Ä‘iá»u hÃ nh: **Windows 10/11**, **Linux**, hoáº·c **macOS**
- RAM tá»‘i thiá»ƒu: **8GB** (khuyáº¿n nghá»‹ 16GB)
- GPU: KhÃ´ng báº¯t buá»™c (cÃ³ GPU sáº½ nhanh hÆ¡n)

### 2ï¸âƒ£ Táº£i source code

- Clone dá»± Ã¡n tá»« GitHub:
  ```bash
  git clone https://github.com/your-repo/ChuyenDoiSo.git
  cd ChuyenDoiSo
  ```
- Hoáº·c táº£i file `.zip` â†’ giáº£i nÃ©n.

### 3ï¸âƒ£ CÃ i Ä‘áº·t dependencies

```bash
pip install -r requirements.txt
```

**Dependencies chÃ­nh:**
- `torch` - PyTorch framework
- `open-clip-torch` - OpenCLIP model
- `gradio` - Web UI framework
- `pillow`, `numpy` - Xá»­ lÃ½ áº£nh vÃ  tÃ­nh toÃ¡n
- `scikit-learn`, `seaborn`, `matplotlib` - ÄÃ¡nh giÃ¡ model

### 4ï¸âƒ£ Kiá»ƒm tra dá»¯ liá»‡u

Äáº£m báº£o cáº¥u trÃºc thÆ° má»¥c nhÆ° sau:
```
data/
â”œâ”€â”€ captions_draft.json    # âœ… File captions tiáº¿ng Viá»‡t (6686 items)
â””â”€â”€ processed/             # âœ… 3003 áº£nh .jpg/.png
```

**Kiá»ƒm tra nhanh:**
```bash
python -c "from pathlib import Path; print(f'Images: {len(list(Path(\"data/processed\").glob(\"*.jpg\")))}'); print(f'Captions: {Path(\"data/captions_draft.json\").exists()}')"
```

### 5ï¸âƒ£ Training model (Fine-tuning + Embeddings)

```bash
python train.py
```

**QuÃ¡ trÃ¬nh training sáº½:**
1. Load OpenCLIP model (~1.46GB - táº£i láº§n Ä‘áº§u sáº½ máº¥t ~15-20 phÃºt)
2. Fine-tune model vá»›i 6686 captions tiáº¿ng Viá»‡t (1 epoch, ~515 batches)
3. TÃ­nh embeddings cho 3003 áº£nh
4. LÆ°u káº¿t quáº£ vÃ o `trained_models/`:
   - `fine_tuned_clip_latest.pt` - Model weights sau fine-tuning
   - `image_embeddings.pkl` - Embeddings cá»§a táº¥t cáº£ áº£nh
   - `text_embeddings_cache.pkl` - Cache query phá»• biáº¿n
   - `training_config.json` - ThÃ´ng tin cáº¥u hÃ¬nh

**Thá»i gian dá»± kiáº¿n:**
- **CPU**: 20-40 phÃºt
- **GPU**: 5-15 phÃºt

### 6ï¸âƒ£ Cháº¡y á»©ng dá»¥ng web

```bash
python app.py
```

**Há»‡ thá»‘ng sáº½:**
1. Load dataset (3003 áº£nh)
2. Load CLIP model
3. Load embeddings tá»« `trained_models/`
4. Khá»Ÿi Ä‘á»™ng Gradio server táº¡i: **http://127.0.0.1:7860**

### 7ï¸âƒ£ Sá»­ dá»¥ng giao diá»‡n tÃ¬m kiáº¿m

<div align="center">
  <table>
    <tr>
      <td align="center" width="50%">
        <img src="docs/search_interface.png" alt="Search Interface" width="400"/><br/>
        <i>HÃ¬nh: Giao diá»‡n tÃ¬m kiáº¿m</i>
      </td>
      <td align="center" width="50%">
        <img src="docs/search_results.png" alt="Search Results" width="400"/><br/>
        <i>HÃ¬nh: Káº¿t quáº£ tÃ¬m kiáº¿m</i>
      </td>
    </tr>
  </table>
</div>

**CÃ¡c bÆ°á»›c tÃ¬m kiáº¿m:**
1. **Nháº­p mÃ´ táº£** - VÃ­ dá»¥: "ngÆ°á»i phá»¥ ná»¯ máº·c Ã¡o dÃ i Ä‘á»"
2. **Äiá»u chá»‰nh tham sá»‘**:
   - Sá»‘ lÆ°á»£ng káº¿t quáº£: 3-30 áº£nh
   - NgÆ°á»¡ng Ä‘á»™ chÃ­nh xÃ¡c: 0.0-0.5 (cÃ ng cao cÃ ng strict)
3. **Nháº¥n "TÃ¬m kiáº¿m"** - Xem káº¿t quáº£ vá»›i Ä‘iá»ƒm similarity

### 8ï¸âƒ£ ÄÃ¡nh giÃ¡ model (Optional)

```bash
python evaluate_model.py
```

Káº¿t quáº£ Ä‘Æ°á»£c lÆ°u trong `evaluation_results/`:
- `confusion_matrix.png` - Ma tráº­n nháº§m láº«n
- `per_category_accuracy.png` - Äá»™ chÃ­nh xÃ¡c theo category
- `classification_report.txt` - BÃ¡o cÃ¡o chi tiáº¿t
- `evaluation_summary.json` - Tá»•ng káº¿t metrics

### 9ï¸âƒ£ VÃ­ dá»¥ query phá»• biáº¿n

**Tiáº¿ng Viá»‡t:**
- "ngÆ°á»i phá»¥ ná»¯ máº·c Ã¡o dÃ i Ä‘á»"
- "tráº» em Ä‘ang vui chÆ¡i"
- "nÃ´ng dÃ¢n Ä‘ang lÃ m viá»‡c trÃªn ruá»™ng"
- "cá»¥ giÃ  Ä‘á»™i nÃ³n lÃ¡"
- "ngÆ°á»i bÃ¡n hÃ ng á»Ÿ chá»£"
- "gia Ä‘Ã¬nh Viá»‡t Nam sum há»p"

**Tiáº¿ng Anh:**
- "elderly woman wearing traditional clothes"
- "vietnamese market seller"
- "smiling person in ao dai"
- "Vietnamese family at home"
- "fisherman working on boat"

### ğŸ”Ÿ Káº¿t thÃºc phiÃªn lÃ m viá»‡c

- ÄÃ³ng trÃ¬nh duyá»‡t hoáº·c nháº¥n **Ctrl+C** trong terminal Ä‘á»ƒ dá»«ng server
- Embeddings Ä‘Ã£ Ä‘Æ°á»£c lÆ°u tá»± Ä‘á»™ng, láº§n sau khÃ´ng cáº§n train láº¡i

âœ… Sau khi hoÃ n táº¥t cÃ¡c bÆ°á»›c trÃªn, báº¡n Ä‘Ã£ cÃ³ thá»ƒ sá»­ dá»¥ng há»‡ thá»‘ng **TÃ¬m kiáº¿m HÃ¬nh áº£nh AI** vá»›i kháº£ nÄƒng hiá»ƒu tiáº¿ng Viá»‡t Ä‘Æ°á»£c fine-tune tá»« 6686 captions!

## âœ¨ 5. TÃ­nh nÄƒng ná»•i báº­t

- ğŸŒ **Äa ngÃ´n ngá»¯**: Há»— trá»£ tÃ¬m kiáº¿m báº±ng Tiáº¿ng Viá»‡t vÃ  Tiáº¿ng Anh
- ğŸ§  **Fine-tuned Model**: ÄÆ°á»£c huáº¥n luyá»‡n vá»›i 6686 captions tiáº¿ng Viá»‡t
- âš¡ **TÃ¬m kiáº¿m tá»©c thÃ¬**: Káº¿t quáº£ hiá»‡n trong vÃ i giÃ¢y
- ğŸ¯ **Äá»™ chÃ­nh xÃ¡c cao**: Similarity score cho má»—i káº¿t quáº£
- ğŸ’¾ **Embedding Cache**: LÆ°u trá»¯ embeddings Ä‘á»ƒ tÄƒng tá»‘c
- ğŸ“Š **Evaluation Tools**: CÃ´ng cá»¥ Ä‘Ã¡nh giÃ¡ vá»›i confusion matrix
- ğŸ’» **CPU/GPU Support**: Cháº¡y Ä‘Æ°á»£c trÃªn cáº£ CPU vÃ  GPU
- ğŸ¨ **Giao diá»‡n Ä‘áº¹p**: Gradio UI hiá»‡n Ä‘áº¡i vá»›i mÃ u sáº¯c Äáº¡i Nam

## ğŸ§  6. Quy trÃ¬nh hoáº¡t Ä‘á»™ng

### `train.py`

1. Load mÃ´ hÃ¬nh OpenCLIP (xlm-roberta-base-ViT-B-32).
2. **Fine-tuning**: Train vá»›i 6686 cáº·p (áº£nh, caption tiáº¿ng Viá»‡t) sá»­ dá»¥ng contrastive learning.
3. **Compute Embeddings**: Encode 3003 áº£nh thÃ nh vector 512 chiá»u â†’ lÆ°u `image_embeddings.pkl`.
4. **Text Cache**: Táº¡o cache cho captions phá»• biáº¿n â†’ `text_embeddings_cache.pkl`.
5. LÆ°u config vÃ  model weights â†’ `trained_models/`.

### `app.py`

1. Load `image_embeddings.pkl` vÃ  cache embeddings.
2. Khá»Ÿi táº¡o Gradio web interface vá»›i giao diá»‡n Äáº¡i Nam.
3. Nháº­n mÃ´ táº£ ngÆ°á»i dÃ¹ng â†’ encode text thÃ nh vector.
4. TÃ­nh cosine similarity vá»›i táº¥t cáº£ áº£nh.
5. Tráº£ vá» top-k áº£nh cÃ³ similarity cao nháº¥t, kÃ¨m Ä‘iá»ƒm sá»‘.

### `evaluate_model.py`

1. DÃ² nhÃ£n tháº­t tá»« tÃªn file áº£nh (Vietnamese_children_, Vietnamese_elderly_, ...).
2. Dá»± Ä‘oÃ¡n category tá»‘t nháº¥t qua CLIP.
3. Táº¡o confusion matrix vÃ  bÃ¡o cÃ¡o precision/recall.
4. Xuáº¥t káº¿t quáº£ vÃ o `evaluation_results/`.

## ğŸ”§ 7. Ghi chÃº & Kháº¯c phá»¥c

### Lá»—i thÆ°á»ng gáº·p:

**âŒ "Trained model not found!"**
- **NguyÃªn nhÃ¢n**: ChÆ°a cháº¡y `train.py`
- **Giáº£i phÃ¡p**: `python train.py` Ä‘á»ƒ táº¡o embeddings

**âŒ "Dataset trá»‘ng!"**
- **NguyÃªn nhÃ¢n**: ThÆ° má»¥c `data/processed/` khÃ´ng cÃ³ áº£nh
- **Giáº£i phÃ¡p**: Copy áº£nh vÃ o `data/processed/`

**âŒ "CUDA out of memory"**
- **NguyÃªn nhÃ¢n**: GPU khÃ´ng Ä‘á»§ RAM
- **Giáº£i phÃ¡p**: Giáº£m `batch_size` trong `train.py` (dÃ²ng 716) hoáº·c dÃ¹ng CPU

**âŒ Download model cháº­m**
- **NguyÃªn nhÃ¢n**: Model 1.46GB táº£i tá»« internet láº§n Ä‘áº§u
- **Giáº£i phÃ¡p**: Äá»£i ~15-20 phÃºt, láº§n sau sáº½ dÃ¹ng cache

### Tips tá»‘i Æ°u:

- ğŸš€ **Training nhanh hÆ¡n**: DÃ¹ng GPU náº¿u cÃ³ (tá»± Ä‘á»™ng detect)
- ğŸ’¾ **Tiáº¿t kiá»‡m RAM**: Giáº£m `batch_size` tá»« 16 xuá»‘ng 8
- ğŸ¯ **TÄƒng Ä‘á»™ chÃ­nh xÃ¡c**: TÄƒng `epochs` trong FineTuneConfig (dÃ²ng 715)
- âš¡ **TÃ¬m kiáº¿m nhanh hÆ¡n**: TÄƒng `min_similarity` Ä‘á»ƒ lá»c káº¿t quáº£

## ğŸ“š 8. TÃ i liá»‡u tham kháº£o

- [OpenCLIP Paper](https://arxiv.org/abs/2103.00020) - CLIP: Learning Transferable Visual Models
- [OpenCLIP GitHub](https://github.com/mlfoundations/open_clip) - Open source implementation
- [Gradio Documentation](https://gradio.app/docs/) - Web UI framework
- [PyTorch Documentation](https://pytorch.org/docs/) - Deep learning framework
- [XLM-RoBERTa Model](https://huggingface.co/xlm-roberta-base) - Multilingual language model

## âœ‰ï¸ 9. LiÃªn há»‡

Náº¿u báº¡n cáº§n trao Ä‘á»•i thÃªm hoáº·c muá»‘n phÃ¡t triá»ƒn má»Ÿ rá»™ng há»‡ thá»‘ng, vui lÃ²ng liÃªn há»‡:

- ğŸ‘¨â€ğŸ’» **TÃ¡c giáº£:** [TÃªn cá»§a báº¡n]
- ğŸ“§ **Email:** [email@example.com]
- ğŸ“± **SÄT:** [0xxxxxxxxx]
- ğŸŒ **GitHub:** [github.com/yourusername]
- ğŸ« **TrÆ°á»ng:** Äáº¡i há»c Äáº¡i Nam - Khoa CÃ´ng nghá»‡ ThÃ´ng tin

<br/>

---

<div align="center">
  <p>Â© 2025 AIoTLab, Faculty of Information Technology, DaiNam University. All rights reserved.</p>
  <p>Made with â¤ï¸ using OpenCLIP & Gradio</p>
</div>
