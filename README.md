<h2 align="center">
    <a href="https://dainam.edu.vn/vi/khoa-cong-nghe-thong-tin">
    ğŸ“  FACULTY OF INFORMATION TECHNOLOGY (DAINAM UNIVERSITY)
    </a>
</h2>
<h2 align="center">
    Há»† THá»NG TÃŒM KIáº¾M HÃŒNH áº¢NH AI
</h2>

<div align="center">
    <p align="center">
        <img src="docs/aiotlab_logo.png" alt="AIoTLab Logo" width="170"/>
        <img src="docs/fitdnu_logo.png" alt="FIT DNU Logo" width="180"/>
        <img src="docs/dnu_logo.png" alt="DaiNam University Logo" width="200"/>
    </p>

[![AIoTLab](https://img.shields.io/badge/AIoTLab-green?style=for-the-badge)](https://www.facebook.com/DNUAIoTLab)
[![Faculty of Information Technology](https://img.shields.io/badge/Faculty%20of%20Information%20Technology-blue?style=for-the-badge)](https://dainam.edu.vn/vi/khoa-cong-nghe-thong-tin)
[![DaiNam University](https://img.shields.io/badge/DaiNam%20University-orange?style=for-the-badge)](https://dainam.edu.vn)

</div>

---

## ğŸ“– 1. Giá»›i thiá»‡u há»‡ thá»‘ng

Há»‡ thá»‘ng **TÃ¬m kiáº¿m HÃ¬nh áº£nh AI** Ä‘Æ°á»£c xÃ¢y dá»±ng dá»±a trÃªn mÃ´ hÃ¬nh **OpenCLIP Ä‘a ngÃ´n ngá»¯ (xlm-roberta-base-ViT-B-32)** nháº±m:

- Há»— trá»£ tÃ¬m kiáº¿m hÃ¬nh áº£nh ngÆ°á»i Viá»‡t Nam thÃ´ng qua mÃ´ táº£ vÄƒn báº£n báº±ng **Tiáº¿ng Viá»‡t** hoáº·c **Tiáº¿ng Anh**.
- Cung cáº¥p giao diá»‡n web hiá»‡n Ä‘áº¡i, thÃ¢n thiá»‡n vá»›i **Gradio**, cho phÃ©p ngÆ°á»i dÃ¹ng nháº­p mÃ´ táº£ vÃ  nháº­n káº¿t quáº£ tá»©c thÃ¬.
- Fine-tune model vá»›i **6686+ captions tiáº¿ng Viá»‡t** Ä‘á»ƒ cáº£i thiá»‡n Ä‘á»™ chÃ­nh xÃ¡c trong ngá»¯ cáº£nh vÄƒn hÃ³a Viá»‡t Nam.

âœ¨ CÃ¡c chá»©c nÄƒng chÃ­nh:

- **Fine-tuning vá»›i Captions**: Huáº¥n luyá»‡n model vá»›i dá»¯ liá»‡u captions tiáº¿ng Viá»‡t Ä‘á»ƒ hiá»ƒu ngá»¯ cáº£nh tá»‘t hÆ¡n.
- **TÃ¬m kiáº¿m thÃ´ng minh**: Nháº­p mÃ´ táº£ chi tiáº¿t (Ã¡o dÃ i, nÃ³n lÃ¡, ngÆ° dÃ¢n, chá»£...) vÃ  nháº­n áº£nh phÃ¹ há»£p nháº¥t.
- **Embedding Cache**: LÆ°u trá»¯ embeddings Ä‘á»ƒ tÄƒng tá»‘c Ä‘á»™ tÃ¬m kiáº¿m.
- **ÄÃ¡nh giÃ¡ Model**: CÃ´ng cá»¥ evaluation vá»›i confusion matrix, accuracy report.

ğŸ¯ Má»¥c tiÃªu há»‡ thá»‘ng:

- **Sá»‘ hÃ³a tra cá»©u hÃ¬nh áº£nh**: Thay tháº¿ tÃ¬m kiáº¿m thá»§ cÃ´ng báº±ng AI thÃ´ng minh.
- **Tá»‘i Æ°u tráº£i nghiá»‡m**: Giao diá»‡n trá»±c quan, káº¿t quáº£ tá»©c thÃ¬ vá»›i Ä‘iá»ƒm similarity.
- **Há»— trá»£ Ä‘a ngÃ´n ngá»¯**: Tiáº¿ng Viá»‡t vÃ  Tiáº¿ng Anh mÃ  khÃ´ng cáº§n dá»‹ch thuáº­t.

## ï¿½ 2. CÃ¡c cÃ´ng nghá»‡ Ä‘Æ°á»£c sá»­ dá»¥ng

- **NgÃ´n ngá»¯:** Python 3.9+
- **MÃ´ hÃ¬nh AI:** OpenCLIP (xlm-roberta-base-ViT-B-32)
- **Framework:** PyTorch, Transformers
- **Giao diá»‡n:** Gradio Web UI
- **PhÃ¢n tÃ­ch:** NumPy, scikit-learn, seaborn, matplotlib
- **Fine-tuning:** Contrastive Learning vá»›i image-caption pairs

<div align="center">

[![Python](https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white)](https://www.python.org/)
[![PyTorch](https://img.shields.io/badge/PyTorch-EE4C2C?style=for-the-badge&logo=pytorch&logoColor=white)](https://pytorch.org/)
[![OpenCLIP](https://img.shields.io/badge/OpenCLIP-412991?style=for-the-badge&logo=openai&logoColor=white)](https://github.com/mlfoundations/open_clip)
[![Gradio](https://img.shields.io/badge/Gradio-FF6F00?style=for-the-badge&logo=gradio&logoColor=white)](https://gradio.app/)
[![NumPy](https://img.shields.io/badge/NumPy-013243?style=for-the-badge&logo=numpy&logoColor=white)](https://numpy.org/)

</div>

## ğŸ“ 3. Cáº¥u trÃºc thÆ° má»¥c

```
ChuyenDoiSo/
â”œâ”€â”€ app.py                          # ğŸŒ á»¨ng dá»¥ng web tÃ¬m kiáº¿m vá»›i Gradio UI
â”œâ”€â”€ train.py                        # ğŸ”¥ Fine-tuning model & tÃ­nh embeddings
â”œâ”€â”€ images_dowload.py               # ï¿½ Script táº£i áº£nh tá»« Pexels API
â”œâ”€â”€ update_captions.py              # ğŸ”„ Cáº­p nháº­t metadata sau lá»c áº£nh
â”œâ”€â”€ models/
â”‚   â””â”€â”€ clip_model.py               # ğŸ¤– Wrapper OpenCLIP (encode image & text)
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ data_loader.py              # ğŸ“‚ Quáº£n lÃ½ dataset vÃ  embeddings
â”‚   â”œâ”€â”€ search_engine.py            # ğŸ” Logic tÃ¬m kiáº¿m vÃ  similarity matching
â”‚   â””â”€â”€ visualizer.py               # ğŸ“Š CÃ´ng cá»¥ hiá»ƒn thá»‹ káº¿t quáº£ Gradio
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ captions_all.json           # ğŸ“ Captions gá»‘c tá»« Pexels
â”‚   â”œâ”€â”€ captions_clean.json         # ğŸ“ Captions sau khi lá»c thá»§ cÃ´ng
â”‚   â”œâ”€â”€ captions_draft.json         # ğŸ“ 6686 captions tiáº¿ng Viá»‡t (fine-tuning)
â”‚   â””â”€â”€ processed/                  # ğŸ–¼ï¸ 3003 áº£nh ngÆ°á»i Viá»‡t Nam Ä‘Ã£ xá»­ lÃ½
â”œâ”€â”€ trained_models/
â”‚   â”œâ”€â”€ image_embeddings.pkl        # ğŸ’¾ Embedding 512-dim cá»§a 3003 áº£nh
â”‚   â”œâ”€â”€ fine_tuned_clip_latest.pt   # ğŸ’¾ Model weights sau fine-tuning
â”‚   â”œâ”€â”€ text_embeddings_cache.pkl   # ğŸ’¾ Cache embedding cho query phá»• biáº¿n
â”‚   â””â”€â”€ training_config.json        # âš™ï¸ Cáº¥u hÃ¬nh training (epochs, batch_size...)
â”œâ”€â”€ .gitignore                      # ğŸš« File ignore cho Git
â”œâ”€â”€ requirements.txt                # ğŸ“¦ Danh sÃ¡ch Python dependencies
â””â”€â”€ README.md                       # ğŸ“– TÃ i liá»‡u hÆ°á»›ng dáº«n dá»± Ã¡n
```

> ğŸ’¡ **LÆ°u Ã½ quan trá»ng**:
>
> - `captions_draft.json`: 6686 captions tiáº¿ng Viá»‡t dÃ¹ng Ä‘á»ƒ fine-tune model
> - `images_dowload.py`: Chá»‰ dÃ¹ng 1 láº§n Ä‘á»ƒ táº£i áº£nh, sau Ä‘Ã³ cÃ³ thá»ƒ bá» qua
> - `update_captions.py`: DÃ¹ng Ä‘á»ƒ Ä‘á»“ng bá»™ metadata sau khi lá»c áº£nh thá»§ cÃ´ng

## âš™ï¸ 4. CÃ¡c bÆ°á»›c cÃ i Ä‘áº·t & sá»­ dá»¥ng

### 1ï¸âƒ£ Chuáº©n bá»‹ mÃ´i trÆ°á»ng

- CÃ i Ä‘áº·t **Python 3.9+** â†’ [Táº£i táº¡i Ä‘Ã¢y](https://www.python.org/downloads/)
- CÃ i Ä‘áº·t **Git** (optional) â†’ [Táº£i táº¡i Ä‘Ã¢y](https://git-scm.com/downloads)
- Há»‡ Ä‘iá»u hÃ nh: **Windows 10/11**, **Linux**, hoáº·c **macOS**
- RAM tá»‘i thiá»ƒu: **8GB** (khuyáº¿n nghá»‹ 16GB)
- GPU: KhÃ´ng báº¯t buá»™c (cÃ³ GPU sáº½ nhanh hÆ¡n)

### 2ï¸âƒ£ Táº£i source code

- Clone dá»± Ã¡n tá»« GitHub:
  ```bash
  git clone https://github.com/your-repo/ChuyenDoiSo.git
  cd ChuyenDoiSo
  ```
- Hoáº·c táº£i file `.zip` â†’ giáº£i nÃ©n.

### 3ï¸âƒ£ CÃ i Ä‘áº·t dependencies

```bash
pip install -r requirements.txt
```

**Dependencies chÃ­nh:**

- `torch` - PyTorch framework
- `open-clip-torch` - OpenCLIP model
- `gradio` - Web UI framework
- `pillow`, `numpy` - Xá»­ lÃ½ áº£nh vÃ  tÃ­nh toÃ¡n
- `scikit-learn`, `seaborn`, `matplotlib` - ÄÃ¡nh giÃ¡ model

### 4ï¸âƒ£ Chuáº©n bá»‹ dá»¯ liá»‡u (Optional - náº¿u chÆ°a cÃ³)

**Náº¿u báº¡n Ä‘Ã£ cÃ³ folder `data/processed/` vá»›i áº£nh sáºµn â†’ Bá» qua bÆ°á»›c nÃ y!**

**Náº¿u muá»‘n tá»± táº£i áº£nh tá»« Ä‘áº§u:**

1. Láº¥y API key tá»« [Pexels](https://www.pexels.com/api/)
2. Cáº­p nháº­t `PEXELS_KEY` trong `images_dowload.py`
3. Cháº¡y script táº£i áº£nh:
   ```bash
   python images_dowload.py
   ```
4. Lá»c áº£nh thá»§ cÃ´ng (xÃ³a áº£nh khÃ´ng phÃ¹ há»£p)
5. Cáº­p nháº­t metadata:
   ```bash
   python update_captions.py
   ```

**Cáº¥u trÃºc dá»¯ liá»‡u cuá»‘i cÃ¹ng:**

```
data/
â”œâ”€â”€ captions_draft.json    # âœ… 6686 items (image name + captions_vi)
â””â”€â”€ processed/             # âœ… 3003 áº£nh .jpg
```

### 5ï¸âƒ£ Kiá»ƒm tra dá»¯ liá»‡u

### 5ï¸âƒ£ Kiá»ƒm tra dá»¯ liá»‡u

Äáº£m báº£o cáº¥u trÃºc thÆ° má»¥c nhÆ° sau:

```
data/
â”œâ”€â”€ captions_draft.json    # âœ… File captions tiáº¿ng Viá»‡t (6686 items)
â””â”€â”€ processed/             # âœ… 3003 áº£nh .jpg/.png
```

**Kiá»ƒm tra nhanh:**

```bash
python -c "from pathlib import Path; import json; print(f'Images: {len(list(Path(\"data/processed\").glob(\"*.jpg\")))}'); data=json.load(open(\"data/captions_draft.json\",encoding=\"utf-8\")); print(f'Captions: {len(data)} items')"
```

### 6ï¸âƒ£ Training model (Fine-tuning + Embeddings)

```bash
python train.py
```

**QuÃ¡ trÃ¬nh training sáº½:**

1. Load OpenCLIP model (~1.46GB - táº£i láº§n Ä‘áº§u sáº½ máº¥t ~15-20 phÃºt)
2. Fine-tune model vá»›i 6686 captions tiáº¿ng Viá»‡t (1 epoch, ~515 batches)
3. TÃ­nh embeddings cho 3003 áº£nh
4. LÆ°u káº¿t quáº£ vÃ o `trained_models/`:
   - `fine_tuned_clip_latest.pt` - Model weights sau fine-tuning
   - `image_embeddings.pkl` - Embeddings cá»§a táº¥t cáº£ áº£nh
   - `text_embeddings_cache.pkl` - Cache query phá»• biáº¿n
   - `training_config.json` - ThÃ´ng tin cáº¥u hÃ¬nh

**Thá»i gian dá»± kiáº¿n:**

- **CPU**: 20-40 phÃºt
- **GPU (CUDA)**: 5-15 phÃºt

> ğŸ’¡ **LÆ°u Ã½**: Láº§n Ä‘áº§u tiÃªn sáº½ táº£i OpenCLIP model tá»« internet (~1.46GB), máº¥t khoáº£ng 15-20 phÃºt tÃ¹y tá»‘c Ä‘á»™ máº¡ng.

### 7ï¸âƒ£ Cháº¡y á»©ng dá»¥ng web

```bash
python app.py
```

**Há»‡ thá»‘ng sáº½:**

1. Load dataset (3003 áº£nh)
2. Load CLIP model
3. Load embeddings tá»« `trained_models/`
4. Khá»Ÿi Ä‘á»™ng Gradio server táº¡i: **http://127.0.0.1:7860**

### 8ï¸âƒ£ Sá»­ dá»¥ng giao diá»‡n tÃ¬m kiáº¿m

<div align="center">
  <p align="center">
    <img src="docs/search_interface.png" alt="Giao diá»‡n tÃ¬m kiáº¿m" width="700"/><br/>
    <i><b>HÃ¬nh 1:</b> Giao diá»‡n tÃ¬m kiáº¿m - Nháº­p mÃ´ táº£ vÃ  Ä‘iá»u chá»‰nh tham sá»‘</i>
  </p>
  <br/>
  <p align="center">
    <img src="docs/search_results.png" alt="Káº¿t quáº£ tÃ¬m kiáº¿m" width="700"/><br/>
    <i><b>HÃ¬nh 2:</b> Káº¿t quáº£ tÃ¬m kiáº¿m - Hiá»ƒn thá»‹ áº£nh kÃ¨m Ä‘iá»ƒm similarity</i>
  </p>
</div>

**CÃ¡c bÆ°á»›c tÃ¬m kiáº¿m:**

1. **Nháº­p mÃ´ táº£** - VÃ­ dá»¥: "ngÆ°á»i phá»¥ ná»¯ máº·c Ã¡o dÃ i Ä‘á»"
2. **Äiá»u chá»‰nh tham sá»‘**:
   - Sá»‘ lÆ°á»£ng káº¿t quáº£: 3-30 áº£nh
   - NgÆ°á»¡ng Ä‘á»™ chÃ­nh xÃ¡c: 0.0-0.5 (cÃ ng cao cÃ ng strict)
3. **Nháº¥n "TÃ¬m kiáº¿m"** - Xem káº¿t quáº£ vá»›i Ä‘iá»ƒm similarity

### 9ï¸âƒ£ VÃ­ dá»¥ query phá»• biáº¿n

**Tiáº¿ng Viá»‡t:**

- "ngÆ°á»i phá»¥ ná»¯ máº·c Ã¡o dÃ i Ä‘á»"
- "tráº» em Ä‘ang vui chÆ¡i"
- "nÃ´ng dÃ¢n Ä‘ang lÃ m viá»‡c trÃªn ruá»™ng"
- "cá»¥ giÃ  Ä‘á»™i nÃ³n lÃ¡"
- "ngÆ°á»i bÃ¡n hÃ ng á»Ÿ chá»£"
- "gia Ä‘Ã¬nh Viá»‡t Nam sum há»p"

**Tiáº¿ng Anh:**

- "elderly woman wearing traditional clothes"
- "vietnamese market seller"
- "smiling person in ao dai"
- "Vietnamese family at home"
- "fisherman working on boat"

### ğŸ”Ÿ Káº¿t thÃºc phiÃªn lÃ m viá»‡c

- ÄÃ³ng trÃ¬nh duyá»‡t hoáº·c nháº¥n **Ctrl+C** trong terminal Ä‘á»ƒ dá»«ng server
- Embeddings Ä‘Ã£ Ä‘Æ°á»£c lÆ°u tá»± Ä‘á»™ng, láº§n sau khÃ´ng cáº§n train láº¡i

âœ… Sau khi hoÃ n táº¥t cÃ¡c bÆ°á»›c trÃªn, báº¡n Ä‘Ã£ cÃ³ thá»ƒ sá»­ dá»¥ng há»‡ thá»‘ng **TÃ¬m kiáº¿m HÃ¬nh áº£nh AI** vá»›i kháº£ nÄƒng hiá»ƒu tiáº¿ng Viá»‡t Ä‘Æ°á»£c fine-tune tá»« 6686 captions!

## âœ¨ 5. TÃ­nh nÄƒng ná»•i báº­t

- ğŸŒ **Äa ngÃ´n ngá»¯**: Há»— trá»£ tÃ¬m kiáº¿m báº±ng Tiáº¿ng Viá»‡t vÃ  Tiáº¿ng Anh
- ğŸ§  **Fine-tuned Model**: ÄÆ°á»£c huáº¥n luyá»‡n vá»›i 6686 captions tiáº¿ng Viá»‡t
- âš¡ **TÃ¬m kiáº¿m tá»©c thÃ¬**: Káº¿t quáº£ hiá»‡n trong vÃ i giÃ¢y vá»›i cosine similarity
- ğŸ¯ **Äá»™ chÃ­nh xÃ¡c cao**: Similarity score kÃ¨m má»—i káº¿t quáº£ (0.0-1.0)
- ğŸ’¾ **Embedding Cache**: LÆ°u embeddings Ä‘á»ƒ khÃ´ng cáº§n tÃ­nh láº¡i
- ï¿½ **Táº£i áº£nh tá»± Ä‘á»™ng**: Script `images_dowload.py` tÃ­ch há»£p Pexels API
- ğŸ”„ **Quáº£n lÃ½ metadata**: `update_captions.py` Ä‘á»“ng bá»™ sau khi lá»c áº£nh
- ğŸ’» **CPU/GPU Support**: Tá»± Ä‘á»™ng detect vÃ  tá»‘i Æ°u theo pháº§n cá»©ng
- ğŸ¨ **Giao diá»‡n Ä‘áº¹p**: Gradio UI hiá»‡n Ä‘áº¡i vá»›i color scheme Äáº¡i Nam

## ğŸ§  6. Quy trÃ¬nh hoáº¡t Ä‘á»™ng

### `images_dowload.py` (Táº£i dá»¯ liá»‡u)

1. Káº¿t ná»‘i vá»›i Pexels API sá»­ dá»¥ng API key.
2. TÃ¬m kiáº¿m áº£nh theo 11 keywords vá» ngÆ°á»i Viá»‡t Nam.
3. Táº£i áº£nh original vÃ  resize vá» 512x512px.
4. LÆ°u captions gá»‘c vÃ o `captions_all.json`.
5. Tá»± Ä‘á»™ng loáº¡i bá» áº£nh trÃ¹ng láº·p báº±ng hash MD5.

### `update_captions.py` (Cáº­p nháº­t metadata)

1. Äá»c `captions_clean.json` (metadata cÅ©).
2. Láº¥y danh sÃ¡ch áº£nh cÃ²n tá»“n táº¡i trong `data/processed/`.
3. Lá»c bá» entries cá»§a áº£nh Ä‘Ã£ xÃ³a thá»§ cÃ´ng.
4. LÆ°u metadata má»›i vÃ o file JSON.

### `train.py` (Training & Fine-tuning)

1. Load mÃ´ hÃ¬nh OpenCLIP (xlm-roberta-base-ViT-B-32).
2. **Fine-tuning**: Train vá»›i 6686 cáº·p (áº£nh, caption tiáº¿ng Viá»‡t) sá»­ dá»¥ng contrastive learning.
3. **Compute Embeddings**: Encode 3003 áº£nh thÃ nh vector 512 chiá»u â†’ lÆ°u `image_embeddings.pkl`.
4. **Text Cache**: Táº¡o cache cho captions phá»• biáº¿n â†’ `text_embeddings_cache.pkl`.
5. LÆ°u config vÃ  model weights â†’ `trained_models/`.

### `app.py`

1. Load `image_embeddings.pkl` vÃ  cache embeddings.
2. Khá»Ÿi táº¡o Gradio web interface vá»›i giao diá»‡n Äáº¡i Nam.
3. Nháº­n mÃ´ táº£ ngÆ°á»i dÃ¹ng â†’ encode text thÃ nh vector.
4. TÃ­nh cosine similarity vá»›i táº¥t cáº£ áº£nh.
5. Tráº£ vá» top-k áº£nh cÃ³ similarity cao nháº¥t, kÃ¨m Ä‘iá»ƒm sá»‘.

### `models/clip_model.py` (CLIP Wrapper)

1. Load OpenCLIP vá»›i model `xlm-roberta-base-ViT-B-32`.
2. Cung cáº¥p phÆ°Æ¡ng thá»©c `encode_image()` vÃ  `encode_text()`.
3. Xá»­ lÃ½ preprocessing (resize, normalize) cho áº£nh.
4. Tokenize vÃ  encode text thÃ nh vector 512-dim.

### `utils/` (Utilities)

- **`data_loader.py`**: Quáº£n lÃ½ ImageDataset, load/save embeddings
- **`search_engine.py`**: ImageSearchEngine vá»›i cosine similarity matching
- **`visualizer.py`**: Format káº¿t quáº£ cho Gradio Gallery

## ğŸ”§ 7. Ghi chÃº & Kháº¯c phá»¥c

### Lá»—i thÆ°á»ng gáº·p:

**âŒ "Trained model not found!"**

- **NguyÃªn nhÃ¢n**: ChÆ°a cháº¡y `train.py`
- **Giáº£i phÃ¡p**: `python train.py` Ä‘á»ƒ táº¡o embeddings

**âŒ "Dataset trá»‘ng!"**

- **NguyÃªn nhÃ¢n**: ThÆ° má»¥c `data/processed/` khÃ´ng cÃ³ áº£nh
- **Giáº£i phÃ¡p**:
  - Option 1: Copy áº£nh cÃ³ sáºµn vÃ o `data/processed/`
  - Option 2: Cháº¡y `python images_dowload.py` Ä‘á»ƒ táº£i tá»« Pexels

**âŒ "CUDA out of memory"**

- **NguyÃªn nhÃ¢n**: GPU khÃ´ng Ä‘á»§ RAM
- **Giáº£i phÃ¡p**: Giáº£m `batch_size` trong `train.py` (dÃ²ng 716) hoáº·c dÃ¹ng CPU

**âŒ Download model cháº­m**

- **NguyÃªn nhÃ¢n**: Model 1.46GB táº£i tá»« Hugging Face láº§n Ä‘áº§u
- **Giáº£i phÃ¡p**:
  - Äá»£i ~15-20 phÃºt Ä‘á»ƒ táº£i xong
  - Láº§n sau model sáº½ dÃ¹ng tá»« cache local (~/.cache/huggingface/)

**âŒ "Pexels API limit exceeded"**

- **NguyÃªn nhÃ¢n**: VÆ°á»£t quota API Pexels (200 requests/hour)
- **Giáº£i phÃ¡p**: Äá»£i 1 giá» hoáº·c Ä‘Äƒng kÃ½ API key má»›i

### Tips tá»‘i Æ°u:

- ğŸš€ **Training nhanh hÆ¡n**: DÃ¹ng GPU vá»›i CUDA náº¿u cÃ³ (auto-detect)
- ğŸ’¾ **Tiáº¿t kiá»‡m RAM**: Giáº£m `batch_size` tá»« 16â†’8 (train.py dÃ²ng 716)
- ğŸ¯ **TÄƒng Ä‘á»™ chÃ­nh xÃ¡c**: TÄƒng `epochs` tá»« 1â†’3 (train.py dÃ²ng 715)
- âš¡ **TÃ¬m kiáº¿m nhanh hÆ¡n**: TÄƒng `min_similarity` tá»« 0.05â†’0.15
- ğŸ“¥ **Táº£i áº£nh nhiá»u hÆ¡n**: Sá»­a `per_page` vÃ  `max_pages` trong images_dowload.py
- ğŸ”„ **Re-train tá»« Ä‘áº§u**: XÃ³a thÆ° má»¥c `trained_models/` trÆ°á»›c khi train

## ğŸ“š 8. TÃ i liá»‡u tham kháº£o

- [OpenCLIP Paper](https://arxiv.org/abs/2103.00020) - CLIP: Learning Transferable Visual Models
- [OpenCLIP GitHub](https://github.com/mlfoundations/open_clip) - Open source implementation
- [Gradio Documentation](https://gradio.app/docs/) - Web UI framework
- [PyTorch Documentation](https://pytorch.org/docs/) - Deep learning framework
- [XLM-RoBERTa Model](https://huggingface.co/xlm-roberta-base) - Multilingual language model
- [Pexels API](https://www.pexels.com/api/documentation/) - Free stock photos API

## âœ‰ï¸ 9. LiÃªn há»‡

Náº¿u báº¡n cáº§n trao Ä‘á»•i thÃªm hoáº·c muá»‘n phÃ¡t triá»ƒn má»Ÿ rá»™ng há»‡ thá»‘ng, vui lÃ²ng liÃªn há»‡:

- ğŸ‘¨â€ğŸ’» **TÃ¡c giáº£:** [Nguyá»…n Quang Hiá»‡p]
- ğŸ“§ **Email:** [quanghiep2342004@gmail.com]
- ğŸ“± **SÄT:** [0396259480]
- ğŸŒ **GitHub:** [github.com/NguyenQuangHiep234]
- ğŸ« **TrÆ°á»ng:** Äáº¡i há»c Äáº¡i Nam - Khoa CÃ´ng nghá»‡ ThÃ´ng tin

- ğŸ‘¨â€ğŸ’» **Äá»“ng tÃ¡c giáº£:** [VÅ© Äá»©c Anh]
- ğŸ“§ **Email:** [anhvuduc9204@gmail.com]
- - ğŸ“± **SÄT:** [********]
- ğŸŒ **GitHub:** [github.com/******]
- ğŸ« **TrÆ°á»ng:** Äáº¡i há»c Äáº¡i Nam - Khoa CÃ´ng nghá»‡ ThÃ´ng tin

<br/>

---

<div align="center">
  <p>Â© 2025 AIoTLab, Faculty of Information Technology, DaiNam University. All rights reserved.</p>
</div>
